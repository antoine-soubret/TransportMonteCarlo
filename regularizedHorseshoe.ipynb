{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "rcParams['figure.dpi'] = 300\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cuda1 = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, nets, nett, mask):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        self.mask = nn.Parameter(mask, requires_grad=False).cuda(1)\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(len(masks))])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(len(masks))])\n",
    "        \n",
    "    def f(self, z):\n",
    "        x = z\n",
    "        log_det_J = z.new_zeros(z.shape[0])\n",
    "        for i in range(len(self.t)):\n",
    "            x_ = x*self.mask[i]\n",
    "            s = self.s[i](x_)*(1 - self.mask[i])\n",
    "            t = self.t[i](x_)*(1 - self.mask[i])\n",
    "            x = x_ + (1 - self.mask[i]) * (x * torch.exp(s) + t)\n",
    "            log_det_J += s.sum(dim=1)\n",
    "        return x, log_det_J\n",
    "\n",
    "    def f_inv(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in reversed(range(len(self.t))):\n",
    "            z_ = self.mask[i] * z\n",
    "            s = self.s[i](z_) * (1-self.mask[i])\n",
    "            t = self.t[i](z_) * (1-self.mask[i])\n",
    "            z = (1 - self.mask[i]) * (z - t) * torch.exp(-s) + z_\n",
    "            log_det_J -= s.sum(dim=1)\n",
    "        return z, log_det_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = int(100),1000\n",
    "\n",
    "X = torch.randn(n,p).cuda(1)\n",
    "\n",
    "d = 5\n",
    "\n",
    "(np.random.normal(5,1,[d])).tolist()\n",
    "\n",
    "w0 = torch.from_numpy(np.array([(np.random.normal(5,1,[d])).tolist() +[0]*(p-d)]).astype(np.float32)).t().cuda(1)\n",
    "\n",
    "\n",
    "psi = (X@w0)[:,0]\n",
    "\n",
    "\n",
    "y = psi + torch.rand(n).cuda(1) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param = p*2 + 4 # needs to be even\n",
    "\n",
    "mask1 = [0,1]* int(total_param/2)\n",
    "mask2 = [1- x for x in mask1]\n",
    "\n",
    "nets = lambda: nn.Sequential(nn.Linear(total_param, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, total_param), nn.Tanh()).cuda(1)\n",
    "nett = lambda: nn.Sequential(nn.Linear(total_param, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, total_param)).cuda(1)\n",
    "masks = torch.from_numpy(np.array([mask1, mask2] * 4).astype(np.float32)).cuda(1)\n",
    "\n",
    "\n",
    "K = 3\n",
    "\n",
    "flow_list = [RealNVP(nets, nett, masks) for _ in range(K)]\n",
    "\n",
    "net_pi = lambda: nn.Sequential(nn.Linear(total_param, 256), nn.LeakyReLU(), nn.Linear(256, 256), \n",
    "                               nn.LeakyReLU(), nn.Linear(256, K), nn.Softmax(dim=1)).cuda(1)\n",
    "\n",
    "flow_pi = net_pi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.rand([batch_size, total_param]).cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 2004])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglik(theta):\n",
    "    idx = 0\n",
    "    idx1 = idx + p\n",
    "    lam2 = torch.log(1+ torch.exp(theta[:,idx: idx1]))\n",
    "    idx = idx1 \n",
    "\n",
    "    idx = idx1\n",
    "    idx1 = idx + p\n",
    "    w = theta[:,idx: idx1]\n",
    "\n",
    "    idx = idx1\n",
    "    idx1 = idx + 1\n",
    "    tau2 = torch.log(1+ torch.exp(theta[:,idx: idx1]))\n",
    "    \n",
    "    idx = idx1\n",
    "    idx1 = idx + 1\n",
    "    c2 = torch.log(1+ torch.exp(theta[:,idx: idx1]))\n",
    "    \n",
    "    idx = idx1\n",
    "    idx1 = idx + 1\n",
    "    sigma2 = torch.log(1+ torch.exp(theta[:,idx: idx1]))\n",
    "    \n",
    "    #prior loss\n",
    "    sigma2_log_prior =  - 100*sigma2 # - (10+1) * torch.log(sigma2) - 0.01/sigma2\n",
    "    tau2_log_prior = - 100*tau2 #- torch.log(1+ tau2/sigma2) - 0.5 * torch.log(sigma2)\n",
    "\n",
    "    c2_log_prior =  - (5.0/2+1) * torch.log(c2) - 250.0/c2    \n",
    "    lam2_log_prior = - torch.log(1+lam2)\n",
    "\n",
    "    tau2_tilde = c2*lam2/(c2+ tau2*lam2)\n",
    "    w_log_prior = - w **2/tau2_tilde/lam2/2.0  - torch.log(tau2_tilde * lam2 )/2.0\n",
    "    \n",
    "    psi = w@X.t()\n",
    "\n",
    "    y_torch = torch.reshape(y,[1,n])\n",
    "\n",
    "    lik = -(y - psi)**2/sigma2/2.0 - torch.log(sigma2)/2.0\n",
    "\n",
    "    total_posterior= (lik).sum(dim=1) + (w_log_prior+ lam2_log_prior).sum(dim=1) + \\\n",
    "        tau2_log_prior.flatten() +  c2_log_prior.flatten() + sigma2_log_prior.flatten()\n",
    "    \n",
    "    return total_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reference(z):\n",
    "    return  0 #- torch.sum((z/ref_sigma).pow(2)/2.0,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for flow in flow_list:\n",
    "    para_list+= list(flow.parameters())\n",
    "    \n",
    "para_list +=  list(flow_pi.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([p for p in para_list if p.requires_grad==True], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import gumbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_rng= gumbel.Gumbel(0,1)\n",
    "\n",
    "\n",
    "def soft_multinomial(Pi):\n",
    "    lam = 0.1\n",
    "    return torch.softmax((Pi.log() + gum_rng.sample(Pi.shape).cuda(1))/lam,1)\n",
    "\n",
    "\n",
    "def soft_multinomial_log(logPi):\n",
    "    lam = 0.1\n",
    "    return torch.softmax((logPi + gum_rng.sample(logPi.shape).cuda(1))/lam,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "best_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z(model, x):\n",
    "    \n",
    "    flow_list = model[0]\n",
    "    flow_pi = model[1]\n",
    "    \n",
    "    Loglik_list = list()\n",
    "    z_list = list()\n",
    "    \n",
    "    for k in range(K):\n",
    "\n",
    "        flow = flow_list[k]\n",
    "        z_cad, logdetJ = flow.f(x)\n",
    "        Loglik_list.append(compLoglik.f(z_cad)+logdetJ + torch.log(flow_pi(z_cad)[:,k]))\n",
    "        z_list.append(z_cad) \n",
    "        \n",
    "    total_loglik = torch.logsumexp(torch.stack(Loglik_list), dim=0)\n",
    "        \n",
    "    loss =  -  total_loglik.mean()\n",
    "        \n",
    "    c = soft_multinomial_log(torch.stack(Loglik_list).t())\n",
    "    z_candidates = torch.stack(z_list)\n",
    "    z = torch.stack( [c[i]@ z_candidates[:,i,:] for i in range(x.shape[0])])\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model= copy.deepcopy([flow_list , flow_pi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([p for p in para_list if p.requires_grad==True], lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 19134.57421875\n",
      "50 1499.3023681640625\n",
      "100 29.828577041625977\n",
      "150 -522.9539794921875\n",
      "200 -790.2320556640625\n",
      "250 -888.21142578125\n",
      "300 -917.061767578125\n",
      "350 -940.9942016601562\n",
      "400 -958.4771728515625\n",
      "450 -966.538818359375\n",
      "500 -981.575439453125\n"
     ]
    }
   ],
   "source": [
    "for t in range(2000):\n",
    "    \n",
    "    beta =  torch.rand([batch_size, total_param]).cuda(1)\n",
    "    \n",
    "    Loglik_list = list()\n",
    "    z_list = list()\n",
    "    \n",
    "    for k in range(K):\n",
    "\n",
    "        flow = flow_list[k]\n",
    "        z_cad, logdetJ = flow.f(beta)\n",
    "        Loglik_list.append(loglik(z_cad)+logdetJ + torch.log(flow_pi(z_cad)[:,k]))\n",
    "#         z_list.append(z_cad) \n",
    "        \n",
    "    g = - torch.logsumexp(torch.stack(Loglik_list), dim=0) + log_reference(beta)\n",
    "        \n",
    "    loss =    g.mean() \n",
    "\n",
    "    \n",
    "    if t%50==0:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    if loss.item()< best_loss:\n",
    "        best_model= copy.deepcopy([flow_list , flow_pi])\n",
    "        best_loss = loss.item()\n",
    "        \n",
    "    if np.isnan(loss.cpu().data.numpy()):    \n",
    "        flow_list,flow_pi = best_model\n",
    "          \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta =  torch.rand([1000, total_param]).cuda(1) \n",
    "\n",
    "Loglik_list = list()\n",
    "z_list = list()\n",
    "\n",
    "for k in range(K):\n",
    "\n",
    "    flow = flow_list[k]\n",
    "    z_cad, logdetJ = flow.f(beta)\n",
    "    Loglik_list.append(loglik(z_cad)+logdetJ + torch.log(flow_pi(z_cad)[:,k]))\n",
    "    z_list.append(z_cad) \n",
    "\n",
    "g = - torch.logsumexp(torch.stack(Loglik_list), dim=0) + log_reference(beta)\n",
    "\n",
    "loss =    g.mean() \n",
    "\n",
    "\n",
    "c = soft_multinomial_log(torch.stack(Loglik_list).t())\n",
    "z_candidates = torch.stack(z_list)\n",
    "\n",
    "z = torch.stack( [c[i]@ z_candidates[:,i,:] for i in range(beta.shape[0])])\n",
    "\n",
    "\n",
    "theta = z\n",
    "\n",
    "idx = 0\n",
    "idx1 = idx + p\n",
    "lam2_post = torch.log(1+ torch.exp(theta[:,idx: idx1]))\n",
    "\n",
    "idx = idx1\n",
    "idx1 = idx + p\n",
    "w_post = theta[:,idx: idx1]\n",
    "\n",
    "idx = idx1\n",
    "idx1 = idx + 1\n",
    "tau2_post = torch.log(1+ torch.exp(theta[:,idx: idx1]))\n",
    "\n",
    "idx = idx1\n",
    "idx1 = idx + 1\n",
    "c2_post = torch.log(1+ torch.exp(theta[:,idx: idx1]))\n",
    "\n",
    "idx = idx1\n",
    "idx1 = idx + 1\n",
    "sigma2_post = torch.log(1+ torch.exp(theta[:,idx: idx1]))\n",
    "\n",
    "\n",
    "w_post = w_post.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(w_post[np.logical_not(np.isnan(w_post[:,0]))]).mean(axis=0)[:d+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5 = pd.DataFrame( {'Value': w_post[:,:10].flatten(order='F'), \n",
    "                    'w': np.repeat(np.arange(1,11),w_post.shape[0]),\n",
    "                    'Method': \"Transport\"\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5_2 = pd.DataFrame( {'Value': np.repeat(w0.cpu().numpy()[:10],w_post.shape[0]) + \n",
    "                      np.random.randn(10*w_post.shape[0])*0.0001, \n",
    "                    'w': np.repeat(np.arange(1,11),w_post.shape[0]),\n",
    "                    'Method': \"Oracle\"\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_long = pd.concat([w5, w5_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3),dpi=500 )       # size in inches\n",
    "\n",
    "ax = sns.violinplot(x=\"w\", y=\"Value\",data=w_long, hue=\"Method\",\n",
    "                    palette=\"Set3\",\n",
    "                    split=True,    scale=\"count\")\n",
    "\n",
    "\n",
    "# plt.savefig(\"horseshoe.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
